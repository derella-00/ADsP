# 의사결정나무

## 가. 정의

- 의사결정나무는 분류함수를 의사결정 규칙으로 이뤄진 **나무 모양으로 그리는 방법**
- 나무구조는 연속적으로 발생하는 의사결정 문제를 **시각화**에 의해 의사결정이 이뤄지는 시점과 성과를 한눈에 볼 수 있게 한다.
- 계산결과가 의사결정나무에 직접 나타나기 때문에 해석이 간편
- 의사결정나무는 주어진 입력값에 대하여 출력값을 예측하는 모형으로 **분류나무**와 **회귀나무모형**이 있다.

</br>

## 나. 예측력과 해석력

- 기대 집단의 사람들 중 가장 많은 반응 보일 **고객의 유치방안을 예측**하고자 하는 경우에는 **예측력에 치중**
- 신용평가에서는 심사 결과 부적격 판정이 나온 경우 고객에게 부적격 **이유를 설명**해야 하므로 **해석력에 치중**

</br>

## 다. 의사결정나무의 활용

### (1) 세분화

- 데이터를 비슷한 특성을 갖는 몇 개의 그룹으로 분할해 그룹별 특성을 발견하는 것이다.

### (2) 분류

- 여러 예측변수들에 근거해 관측개체의 목표변수 범주를 몇 개의 등급으로 분류하고자 하는 경우

### (3) 예측

- 자료에서 규칙을 찾아내고 이를 이용해 미래의 사건을 예측하고자 하는 경우

### (4) 차원축소 및 변수선택

- 매우 많은 수의 예측변수 중에서 목표변수에 큰 영향을 미치는 변수들을 골라내고자 하는 경우

### (5) 교호작용효과의 파악

- 여러 개의 예측변수들을 결합해 작용하는 규칙을 파악하고자 하는 경우

- 범주의 병합 또는 연속형 변수의 이산화

</br>

## 라. 의사결정나무의 특징

### 장점

- 결과를 누구에게나 설명하기 용이
- 모형을 만드는 방법이 계산적으로 복잡 ❌
- 대용량 데이터에서도 빠르게 만들 수 있다
- 비정상 잡음 데이터에 대해서도 민감함 없이 분류
- 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않는다.
- 설명변수나 목표변수에 수치형변수와 범주형변수를 모두 사용 가능
- 모형 분류 정확도가 높다

### 단점

- 새로운 자료에 대한 **과대적합이 발생할 가능성**이 높다
- 분류 경계선 부근의 자료값에 대해서 오차가 크다
- 설명변수 간의 중요도를  판단하기 쉽지 않다.

</br>

## 마. 의사결정나무의 분석 과정

의사결정나무의 형성 과정은 크게 **성장(growing), 가지치기(pruning), 타당성 평가, 해석 및 예측**

### (1) 성장 단계

- 각 마디에서 적잘한 최적의 `분리규칙`(splitting rule)을 찾아서 나무를 성장시키는 과정, 적절한 `정지규칙`(stopping rule)을 만족하면 중단

### (2) 가지치기 단계

- 오차를 크게 할 위험이 높거나 부적절한 추론규칙을 가지고 있는 가지 또는 불필요한 가지를 `제거`하는 단계

### (3) 타당성 평가 단계

- `이익도표`(gaun chart), `위험도표`(risk chart), 혹은 시험자료를 이용하여 의사결정나무를 평가

### (4) 해석 및 예측 단계

- 구축된 나무모형을 해석하고 예측모형을 설정한 후 예측에 적용하는 단계

</br>

## 바. 나무의 성장

- 훈련자료를$(x_1, y_1) \quad i=1,2,\cdots,n$으로 나타내자 여기서 $x_i=(x_{i1},\cdots,x_{ip})$
- 나무모형의 성장과정은 $x$들로 이루어진 입력 공간을 재귀적으로 분할하는 과정이다.
    
    $R_1(j,A)=x_j\in A, \quad R_2=(j,A^c)=x_j\in A^c$

</br>

### (1) 분리규칙(splitting rule)

- 분리 변수가 연속형인 경우 : $A = x_j \le s$

- 분리변수가 범주형 {1, 2, 3, 4}인 경우, $A=1,\ 2,\ 4$와 $A^c=3$으로 나눌 수 있다.
- 최적 분할의 결정은 불순도 감소량을 가장 크게 하는 분할이다.

    $\vartriangle i(t)=i(t)-p_Li(t_L)-p_R(i_R),\quad i(t)=\sum_{i\in t}(y_i-\bar{y_t})^2$
    
- 각 단계에서 최적 분리기준에 의한 분할을 찾은 다음 각 분할에 대하여도 동일한 과정을 반복한다.

### (2) 분리기준

- 연속형 목표변수 (분리 변수가 연속형인 경우 : $A = x_j \le s$)
    
    
    | 기준값 | 분리기준 |
    | --- | --- |
    | 분산분석에서 F 통계량 | P값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성 |
    | 분산의 감소량 | 분산의 감소량을 최대화 하는 기준의 최적분리에 의해서 자식마디를 형성 |

</br>

- 이산형 목표변수 (분리변수가 범주형 {1, 2, 3, 4}인 경우, $A=1,\ 2,\ 4$와 $A^c=3$으로 나눌 수 있다.)
    
    
    | 기준값 | 분리기준 |
    | --- | --- |
    | 카이제곱 통계량 p값 | P값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성 |
    | 지니 지수 | 지니 지수를 감소시켜주는 예측변수와 그 때의 최적분리에 의해서 자식마디를 선택 |
    | 엔트로피 지수 | 엔트로피 지수가 가장 작은 예측 변수와 이때의 최적분리에 의해 자식마디를 형성 |


**3) 정지규칙(stopping rule)**

- 더 이상 분리가 일어나지 않고, 현재의 마디가 끝마디가 되도록 하는 규칙
- 정지기준(stopping criterion) : 의사결정나무의 깊이(depth)를 지정, 끝마디의 레코드 수의 최소 개수를 지정한다.

### 사. 나무의 가지치기(Pruning)

- 과대적합되어 현실 문제에 적응할 수 있는 적절한 규칙이 나오지 않는 현상을 방지하지 위해 사용되는 방법
- 너무 큰 나무 모형 → 자료를 과대적합, 너무 작은 나무 모형 → 과소적합할 위험
- 나무의 크기를 모형의 복잡도로 볼 수 있으며 최적의 나무 크기는 자료로부터 추정하게 된다.
일반적으로 **자료가 일정 수(가령 5)** 이하 일 때 분할을 정지하고 **비용-복잡도 가지치기(cost complexity pruning)를 이용**하여 성장시킨 나무를 가지치기하게 된다.
    
    ![Untitled](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/Untitled%202.jpeg)
    

# 4. 불순도의 여러 가지 측도

- 목표변수가 **범주형 변수**인 의사결정나무의 분류규칙을 선택하기 위해서는 **카이제곱 통계량, 지니지수, 엔트로피 지수를 활용**한다.
    
    
    **1) 카이제곱 통계량**
    
    - 각 셀에 대한 ((실제도수-기대도수)의 제곱/기대도수)의 합
        - 기대도수 = 열의 합계 x 합의 합계 / 전체합계
            
            ### $z^2=\sum_{i=1}^k\frac{(B_i-O_i)^2}{B_i}$ ($k$ : 범주의 수, $O_i$ : 실제도수, $B_i$ : 기대도수)
            
    - p-value는 높을수록 자식노드 간의 이질성이 낮다. → 구분이 잘 안됨, 분류하기 어려움
    - 낮은 p-value 값을 선택해서 이질성이 큰 변수를 분류 (이질성이 크면 같은 게 많이 있다는 얘기)
    
    **2) 지니지수**
    
    - 노드의 **불순도**를 나타내는 값
    - 지니지수의 값이 클수록 이질적(Diversity)이며 **순수도(Purity)**가 낮다고 볼 수 있다. 순수도가 낮다 → 분류 잘 안됨
        
        ### $Gini(T) = 1-\sum_{l=1}^kP_l^2$
        
        ![Untitled](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/Untitled%203.jpeg)
        
        ![Screen Shot 2022-10-23 at 2.25.07 PM.png](R%20db0bd2b276354cd2948e40dd96bccfd7/Calender%2080a06459ebe54eb9b01c66cca9c95d08/ADsP%2029%E1%84%92%E1%85%AC%2045d654b34d314e7c991a5d3886acc7a3/Screen_Shot_2022-10-23_at_2.25.07_PM.png)
        
        - 지니지수
            1. 지니 지수 값이 클수록 이질적이며 순수도가 낮다고 할 수 있다.
            순수도가 낮다 → 분류가 잘 안된다.
                
                지니 지수 값이 작을수록 동질적이며 순수도가 높다고 할 수 있다. 
                지니 지수 값이 작다? 1에서 많이 뺐다 → 순수도가 높다 → 분류가 잘 된다.
                
        
        ![Screen Shot 2022-10-24 at 4.46.40 PM.png](R%20db0bd2b276354cd2948e40dd96bccfd7/Calender%2080a06459ebe54eb9b01c66cca9c95d08/ADsP%2027%E1%84%92%E1%85%AC%20637a6f9834a14404b3a282ba68d412a5/Screen_Shot_2022-10-24_at_4.46.40_PM.png)
        
        - 의사결정나무 분류규칙 불순도 측도

**3) 엔트로피 지수**

- 무질서 정도에 대한 측도
- 엔트로피 지수의 값이 클수록 **순수도**가 낮다고 볼 수 있다. 고로 지수의 값이 낮을 수록 분류 잘 됨.
- 엔트로피 지수가 가장 작은 예측변수와 이때의 최적분리 규칙에 의해 자식마디를 형성한다.
    
    ### $Entropy(T)=-(\sum^k_{l=1}p_l\log_2p_l)$
    
    ![Untitled](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/Untitled%204.jpeg)
    
    ![Untitled](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/Untitled%205.jpeg)
    
    ![Untitled](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/Untitled%206.jpeg)
    

# 5. 의사결정나무 알고리즘

**공통 개념:**
하향식 기법이 사용되며, 각 진행 단계에서는 주어진 데이터 집합을 가장 적합한 기준으로 분할하는 변수값이 선택된다.

### 가. CART(Classification and Regression Tree)

- 불손도의 측도로 출력(목적) 변수가 범주형일 경우 지니지수를 이용, 연속형인 경우 분산을 이용한 이진분리(binary split)를 사용한다.
- 개별 입력변수 뿐만 아니라 입력변수들의 선형결합들 중에서 최적의 분리를 찾을 수 있다.

### 나. C4.5와 C5.0

- CART와는 다르게 각 마디에서 다지분리(multiple split)가 가능하며 범주형 입력변수에 대해서는 범주의 수만큼 분리가 일어난다.
- 불순도의 측도로는 엔트로피 지수를 활용한다.

### 다. CHAID(CHi-squared Automatic Interaction Detection)

- 가지치기를 하지 않고 적당한 크기에서 나무모형의 성장을 주이하며 입력변수가 반드시 범주형 변수이어야 한다.
- 분순도의 측도로는 카이제곱 통계량을 사용한다.

# 6. 의사결정나무 예시

### 가. party 패키지를 이용한 의사결정나무

- party 패키지는 의사결정나무를 사용하기 편한 다양한 분류 패키지 중 하나이다.
- 분실값을 잘 처리하지 못하는 문제를 갖고 있는 것이 단점
- tree에 투입된 데이터가 표시가 되지 않거나 predict가 실패하는 경우 문제가 발생할 수 있다.
    
    1) iris data 를 이용한 분석
    
    - iris data의 30%는 test data, 70% training data로 생성한다.
    
    ![Untitled](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/Untitled%207.jpeg)
    
    2) train.data를 이용하여 모형생성
    
    ![Untitled](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/Untitled%208.jpeg)
    
    ![Untitled](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/Untitled%209.jpeg)
    
    ![IMG_5853.jpg](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/IMG_5853.jpg)
    
    ![IMG_5854.jpg](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/IMG_5854.jpg)
    

![Untitled](5%E1%84%8C%E1%85%A1%E1%86%BC%202%E1%84%8C%E1%85%A5%E1%86%AF%20%E1%84%87%E1%85%AE%E1%86%AB%E1%84%85%E1%85%B2%E1%84%87%E1%85%AE%E1%86%AB%E1%84%89%E1%85%A5%E1%86%A8%2036dc5f43f11849419cf3770b6f6455e1/Untitled%2010.jpeg)